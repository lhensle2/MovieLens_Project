---
title: "MovieLens"
author: "Luke Hensley"
date: "6/10/2019"
output:
  pdf_document: default
  html_document: default
---

I. Introduction

Three models ("Simple Average", "Movie_Effect" and "Movie+User_Effect") were developed and assessed using RMSE for this project. The best model, "Movie + User_Effect Model" (RMSE 0.8426), is run directly against the validation set to predict movie ratings. The RMSE result on the validation dataset (0.8294) is lower than that of the test dataset (0.8426). THis would suggest that the model is likely a good prediction model.

II. Load Data
```{r,echo=FALSE}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)
ratings <- read.table(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
col.names = c("userId", "movieId", "rating", "timestamp"))
movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId], title = as.character(title), genres = as.character(genres))
movielens <- left_join(ratings, movies, by = "movieId")
set.seed(1)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]
validation <- temp %>%
semi_join(edx, by = "movieId") %>%
semi_join(edx, by = "userId")
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)
rm(dl, ratings, movies, test_index, temp, movielens, removed)
```
III. Training and Testing dataset
Datasets are derived using edx set: 80% sample for training, 20% sample for testing.
```{r}
set.seed(1)
train_index <- createDataPartition(y = edx$rating, times = 1, p = 0.8, list = FALSE)
train_set <- edx[train_index,]
temp <- edx[-train_index,]
test_set <- temp %>%
semi_join(train_set, by = "movieId") %>%
semi_join(train_set, by = "userId")
removed <- anti_join(temp, test_set)
train_set <- rbind(train_set, removed)
rm(temp, removed)
```
IV. Evaluate Algorithm

The following is used to test the three algorithms.
```{r}
RMSE <- function(true_ratings, predicted_ratings){
sqrt(mean((true_ratings - predicted_ratings)^2))
}
```
1st model: Simple Average Model 

```{r}
mu_hat <- mean(train_set$rating)
model_1_rmse <- RMSE(test_set$rating, mu_hat)
rmse_results <- data_frame(Model = "Simple Average", RMSE = model_1_rmse)
rmse_results%>%knitr::kable()
```
2nd model: Movie_Effect Model 

```{r}
mu <- mean(train_set$rating)
movie_avgs <- train_set %>%
  group_by(movieId) %>%
  summarize(b_i = mean(rating - mu))

predicted_ratings <- mu + test_set %>%
left_join(movie_avgs, by='movieId') %>%
.$b_i
model_2_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results,
data_frame(Model="Movie_Effect",
RMSE = model_2_rmse ))
rmse_results %>% knitr::kable()
```
The RMSE shows the 2nd model is an improvement from the 1st.

3rd model: Movie+User_Effect Model 

```{r}
user_avgs <- test_set %>%
left_join(movie_avgs, by='movieId') %>%
group_by(userId) %>%
summarize(b_u = mean(rating - mu - b_i))

predicted_ratings <- test_set %>%
left_join(movie_avgs, by='movieId') %>%
left_join(user_avgs, by='userId') %>%
mutate(pred = mu + b_i + b_u) %>%
.$pred
model_3_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results,
data_frame(Model="Movie + User_Effect",
RMSE = model_3_rmse ))
rmse_results %>% knitr::kable()
```
RMSE is further reduced using the 3rd model.

V. Evaluate validation set

Based on the above results, the best model, "Movie + User_Effect Model", is selected and run against the validation set. The RMSE of the validation set is 0.8294.
```{r}
user_avgs_validation <- validation %>%
left_join(movie_avgs, by='movieId') %>%
group_by(userId) %>%
summarize(b_u = mean(rating - mu - b_i))
predicted_ratings <- validation %>%
left_join(movie_avgs, by='movieId') %>%
left_join(user_avgs_validation, by='userId') %>%
mutate(pred = mu + b_i + b_u) %>%
.$pred
model_rmse_validation <- RMSE(predicted_ratings, validation$rating)
model_rmse_validation
```
VI. Conclusion

In this project, three models ("Simple Average", "Movie_Effect" and "Movie+User_Effect") were developed and assessed using their RMSE. The best model, "Movie + User_Effect Model" (RMSE of 0.8426), was run directly against the validation set. The RMSE of the validation dataset was 0.8294, and is lower than that of the test dataset (0.8426). This suggests that the model is likely a good prediction model.

Link to GitHub: https://github.com/lhensle2/MovieLens_Project.git
